        ---
title: "Data Science Capstone Project: NLP"
author: "Steffen Hartleib"
date: "July 21, 2015"
output: html_document
---

### overview
This the first milestone report for the Coursera Data Science Capstone Project.    
The project assignment is to create an application that takes a string of words from a user and predicts the next word.  
This initial report explores the data and briefly describes the next steps in building the model.  


### the raw data

There are three large sets of English language data:  

n_US.news.txt -- 196M    
en_US.blogs.txt -- 196M    
en_US.twitter.txt -- 196M    

These files are too large to process efficently, so I created a smaller sample set.


###  creating the sample sets

For each file I drew a random sample of 1% of the lines:


## line counts and size of sample files

```{r,echo = FALSE, eval = TRUE, message = FALSE}
setwd("/Users/steffenhartleib/Google Drive/Capstone/corpus/text")
News<- length(readLines("NewsSample.txt"))
Blogs <- length(readLines("BlogSample.txt"))
Twitter<- length(readLines("TwitterSample.txt"))
lineCounts <- as.data.frame(cbind(News,Blogs,Twitter))

News <- file.size("NewsSample.txt")/1024/1024
Blogs <- file.size("BlogSample.txt")/1024/1024
Twitter <- file.size("TwitterSample.txt")/1024/1024
size <- as.data.frame(cbind(News,Blogs,Twitter))
sampleFiles <-rbind(size,round(lineCounts,0))

row.names(sampleFiles) <- c('Size(MB)','LineCount')
sampleFiles$News <- round(sampleFiles$News)
sampleFiles$Blogs <- round(sampleFiles$Blogs)
sampleFiles$Twitter<- round(sampleFiles$Twitter)
sampleFiles
```

These sample files make up the corpora for the rest of the analysis.

### cleaning the data
removed punctuation      
removed any non alphabetical charcters        
kept dashes and apostrophes within words   
added periods back to most frequent abreviations    
converted all words to lower case      
removed profanties  

### tokinizing the data sets
created frequency tables of:   
single words    
2 word phrases (bigrams)   
3 word phrases(trigrams)    
 

### single words

The corpus of sample files includes:    
1.3 Million instances made up of about 72k distinct words    
The average frequency is 19    
The the median frequency is 2    
The data is highly skewed by a few highfrequency words such as "the", "and".    
Since the assignemnt is to predict the next word, I did not remove these
stop words. 

#### Here are the 25 most frequent words

```{r,echo = FALSE, eval = TRUE, message = FALSE}
require(dplyr)
require(ggplot2)
options(scipen=999)
setwd("/Users/steffenhartleib/Google_Drive/Capstone/corpus/clean/tokens")
df.freq <- read.csv("words.csv",head = TRUE)
```

```{r,echo = FALSE, eval = TRUE, message = FALSE}
wFreq = arrange(df.freq, desc(freq))%>%
        mutate(cumeFreqPerc = round(cumsum(freq)/sum(freq),2))%>%
        mutate(cumeFreq = cumsum(freq))%>%
        #filter(cumeFreqPerc <= .50)%>%
        arrange(desc(freq))
#summary(wFreq)
```
```{r,echo = FALSE, eval = TRUE, message = FALSE}
# top 25 words plot

w25 <- ggplot(wFreq[1:25,], aes(x = reorder(wFreq[1:25,]$terms, wFreq[1:25,]$freq), y = freq))
w25 <- w25 + geom_bar(stat = 'identity') + coord_flip() + xlab("") +ggtitle("Top 25 words by frequency")
w25
```

#### frequency distribution
To make the chart more legible I zoomed in on frequencies between 1,000 and 10,000. Still the data remains highl skewed.

```{r,echo = FALSE, eval = TRUE, message = FALSE}
# Word frequency


wf <- ggplot(wFreq[wFreq$freq > 1000 & wFreq$freq <10000,], aes(freq))
wf <- wf + geom_histogram() + ggtitle("Frequency of words appearing between 1,000 and 10,000 times")
wf
```


#### How many unique words does it take to cover 50% and 90% of the instances?  
Because the data is so heavily skewed towards high frequency words, it takes only 375 words to cover 50% of the instances. 10,000 words will get you to 90% coverage:
```{r, echo =FALSE, eval = TRUE, message = FALSE}
wFreqUn <- mutate(wFreq, cumeTerms = rownames(wFreq))%>%
        group_by(cumeFreqPerc)%>%
        summarize(UniqueTerms = n())
wFreqUn = mutate(wFreqUn,cumeUniqueTerms = cumsum(UniqueTerms))
#50% of instances
pWU50 <- ggplot(wFreqUn, aes(y = cumeUniqueTerms, x=cumeFreqPerc))
pWU50 <- pWU50 + geom_bar(stat = 'identity') + ggtitle("375 words cover 50% percent of  instances") +
        ylab("Number of unique words") + xlab("Percentage of total instances")+
        scale_x_continuous("Frequency", limits = c(0, 1), breaks = seq(0, 
    1, .1)) + scale_y_continuous("Number of unique words", limits = c(0, 75000), breaks = seq(0, 75000, 5000)) + xlim(0,0.5) + ylim(0,500)
pWU50
````

```{r, echo =FALSE, eval = TRUE, message = FALSE}
#90% of instances
pWU90 <- ggplot(wFreqUn, aes(y = cumeUniqueTerms, x=cumeFreqPerc))
pWU90 <- pWU90 + geom_bar(stat = 'identity') + ggtitle("10,000 words cover 90% percent of instances") +
        ylab("Number of unique words") + xlab("Percentage of total instances")+
        scale_x_continuous("Frequency", limits = c(0, 1), breaks = seq(0, 
    1, .1)) + scale_y_continuous("Number of unique words", limits = c(0, 75000), breaks = seq(0, 75000, 5000))
pWU90
```

### two word phrases (biGrams)

```{r, echo = FALSE, eval = TRUE, message = FALSE}
setwd("/Users/steffenhartleib/Google_Drive/Capstone/corpus/clean/tokens")
biGramFreq <- read.csv("biGrams.csv", header = TRUE)
biGramFreq <- arrange(biGramFreq, desc(freq))%>%
        mutate(cumeFreqPerc = round(cumsum(freq)/sum(freq),2))%>%
        mutate(cumeFreq = cumsum(freq))%>%
        #filter(cumeFreqPerc <= .50)%>%
        arrange(desc(freq))
#summary(biGramFreq)
```

There are 1.6 million instances of 680k unique phrases  
Then mean frequency is 2.4 and the median is 1  
This means data is significantly less skewed than single words.

#### Here are the 25 most frequent biGrams:


```{r, echo =FALSE, eval = TRUE, message = FALSE}
#top 25 bigrams
biGramFreq = arrange(biGramFreq, desc(freq))%>%
        mutate(cumeFreqPerc = round(cumsum(freq)/sum(freq),2))%>%
        mutate(cumeFreq = cumsum(freq))%>%
        #filter(cumeFreqPerc <= .50)%>%
        arrange(desc(freq))

bG25 <- ggplot(biGramFreq[1:25,], aes(x = reorder(biGramFreq[1:25,]$terms, biGramFreq[1:25,]$freq), y = freq))
bG25 <- bG25 + geom_bar(stat = 'identity') + coord_flip() + xlab("") + ylab("Frequency") + 
        ggtitle("Top 25 biGrams by frequency")
bG25
```

#### frequency distribution:
Again, to make the chart more legible, I zoomed in on frequencies over 1,000
```{r, echo =FALSE, eval = TRUE, message = FALSE}
# biGram Frequencies
bf <- ggplot(biGramFreq[biGramFreq$freq > 1000,], aes(freq))
bf <- bf + geom_histogram(binwidth = 500)
bf
```

#### How many unique biGrams will cover 50% and 90% of the instances?
Since the biGrams much less skewed than the single words it takes 50,000 unique bigrams to cover 50% of the instances, and a whopping 500,000 to cover 90% of the instances. (better wrap up the exploratory analysis and start working on the model...)
```{r, echo =FALSE, eval = TRUE, message = FALSE}
# unique biGrams covering  x% of total instances
biGramFreqUn <-  mutate(biGramFreq,cumeTerms = rownames(biGramFreq))%>%
        group_by(cumeFreqPerc)%>%
        summarize(UniqueTerms = n())      
biGramFreqUn = mutate(biGramFreqUn,cumeUniqueTerms = cumsum(UniqueTerms))
#print.data.frame(biGramFreqUn)

bG <- ggplot(biGramFreqUn, aes(y = cumeUniqueTerms, x=cumeFreqPerc))
bG <- bG + geom_bar(stat = 'identity') + 
        ggtitle("Unique biGrams covering percentage of instances ") +
        ylab("Number of unique biGrams") + xlab("Percentage of total instances") + scale_x_continuous("Percentage of instances", limits = c(0, 1), breaks = seq(0, 1, .1)) +  scale_y_continuous("Number of unique biGrams (2 word phrases)", limits = c(0, 750000), breaks = seq(0, 750000, 50000))
bG
```

### three word phrases (triGrams)
There are 730k unique triGrams, and 1.20 million instances.    
The mean frequency is 1.2 and the median of 1. So while the data is less skewed, it's also very narrowly distributed around the mean of one. In other words, most phrases appear only once.   

```{r, echo =FALSE, eval = TRUE, message = FALSE}
# triGrams
setwd("/Users/steffenhartleib/Google_Drive/Capstone/corpus/clean/tokens")
triGramFreq <- read.csv("triGrams.csv", header = TRUE)
triGramFreq = arrange(triGramFreq, desc(freq))%>%
        mutate(cumeFreqPerc = round(cumsum(freq)/sum(freq),2))%>%
        mutate(cumeFreq = cumsum(freq))  %>%
        arrange(desc(freq))
```

```{r, echo =FALSE, eval = FALSE, message = FALSE}
summary(triGramFreq)
```
#### here are the top 25 trigrams:
```{r, echo =FALSE, eval = TRUE, message = FALSE}
# top 25 triGrams plot
tG25 <- ggplot(triGramFreq[1:25,], aes(x = reorder(triGramFreq[1:25,]$terms, triGramFreq[1:25,]$freq), y = freq))
tG25 <- tG25 + geom_bar(stat = 'identity') + coord_flip() + xlab("") + ylab("Frequency") + 
        ggtitle("Top 25 triGrams by frequency")
tG25
```

#### frequency distribution

```{r, echo =FALSE, eval = TRUE, message = FALSE}
tf <- ggplot(triGramFreq, aes(freq))
tf <- tf + geom_histogram(binwidth = 1) + xlim(0,20)
tf
```
  
#### how many unique triGrams will cover 50% and 90% of the instances?
Once you cover 25% of the unique instances the relationship becomes linear. 
This means we'll need to include roughly 50% unique triGrams to cover 50% of the instances, 75% of unique triGrams to cover 75% of instances etc. 

```{r, echo =FALSE, eval = TRUE, message = FALSE}
# unique biGrams covering  x% of total instances
triGramFreqUn <-  mutate(triGramFreq,cumeTerms = rownames(triGramFreq))%>%
        group_by(cumeFreqPerc)%>%
        summarize(UniqueTerms = n())      
triGramFreqUn = mutate(triGramFreqUn,cumeUniqueTerms = cumsum(UniqueTerms))
#print.data.frame(triGramFreqUn)

tG <- ggplot(triGramFreqUn, aes(y = cumeUniqueTerms, x=cumeFreqPerc))
tG <- tG + geom_bar(stat = 'identity') + 
        ggtitle("triGrams to cover x % of instances in sample") +
        ylab("Number of unique triGrams") + xlab("Percentage of total instances")
tG
```




